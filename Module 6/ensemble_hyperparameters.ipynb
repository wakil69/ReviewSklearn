{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c90308",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "In the previous section, we did not discuss the hyperparameters of random\n",
    "forest and histogram gradient-boosting. This notebook gives crucial\n",
    "information regarding how to set them.\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">For the sake of clarity, no nested cross-validation is used to estimate the\n",
    "variability of the testing error. We are only showing the effect of the\n",
    "parameters on the validation set.</p>\n",
    "</div>\n",
    "\n",
    "We start by loading the california housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84090780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bdd96",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "The main parameter to select in random forest is the `n_estimators` parameter.\n",
    "In general, the more trees in the forest, the better the generalization\n",
    "performance would be. However, adding trees slows down the fitting and prediction\n",
    "time. The goal is to balance computing time and generalization performance\n",
    "when setting the number of estimators. Here, we fix `n_estimators=100`, which\n",
    "is already the default value.\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Tuning the <tt class=\"docutils literal\">n_estimators</tt> for random forests generally result in a waste of\n",
    "computer power. We just need to ensure that it is large enough so that doubling\n",
    "its value does not lead to a significant improvement of the validation error.</p>\n",
    "</div>\n",
    "\n",
    "Instead, we can tune the hyperparameter `max_features`, which controls the\n",
    "size of the random subset of features to consider when looking for the best\n",
    "split when growing the trees: smaller values for `max_features` lead to\n",
    "more random trees with hopefully more uncorrelated prediction errors. However\n",
    "if `max_features` is too small, predictions can be too random, even after\n",
    "averaging with the trees in the ensemble.\n",
    "\n",
    "If `max_features` is set to `None`, then this is equivalent to setting\n",
    "`max_features=n_features` which means that the only source of randomness in\n",
    "the random forest is the bagging procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08156ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this case, n_features=8\n"
     ]
    }
   ],
   "source": [
    "print(f\"In this case, n_features={len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd7b58",
   "metadata": {},
   "source": [
    "We can also tune the different parameters that control the depth of each tree\n",
    "in the forest. Two parameters are important for this: `max_depth` and\n",
    "`max_leaf_nodes`. They differ in the way they control the tree structure.\n",
    "Indeed, `max_depth` enforces growing symmetric trees, while `max_leaf_nodes`\n",
    "does not impose such constraint. If `max_leaf_nodes=None` then the number of\n",
    "leaf nodes is unlimited.\n",
    "\n",
    "The hyperparameter `min_samples_leaf` controls the minimum number of samples\n",
    "required to be at a leaf node. This means that a split point (at any depth) is\n",
    "only done if it leaves at least `min_samples_leaf` training samples in each of\n",
    "the left and right branches. A small value for `min_samples_leaf` means that\n",
    "some samples can become isolated when a tree is deep, promoting overfitting. A\n",
    "large value would prevent deep trees, which can lead to underfitting.\n",
    "\n",
    "Be aware that with random forest, trees are expected to be deep since we are\n",
    "seeking to overfit each tree on each bootstrap sample. Overfitting is\n",
    "mitigated when combining the trees altogether, whereas assembling underfitted\n",
    "trees (i.e. shallow trees) might also lead to an underfitted forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e017b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"max_features\": [1, 2, 3, 5, None],\n",
    "    \"max_leaf_nodes\": [10, 100, 1000, None],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_jobs=2),\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_iter=10,\n",
    "    random_state=0,\n",
    "    n_jobs=2,\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333afe9a",
   "metadata": {},
   "source": [
    "We can observe in our search that we are required to have a large number of\n",
    "`max_leaf_nodes` and thus deep trees. This parameter seems particularly\n",
    "impactful with respect to the other tuning parameters, but large values of\n",
    "`min_samples_leaf` seem to reduce the performance of the model.\n",
    "\n",
    "In practice, more iterations of random search would be necessary to precisely\n",
    "assert the role of each parameters. Using `n_iter=10` is good enough to\n",
    "quickly inspect the hyperparameter combinations that yield models that work\n",
    "well enough without spending too much computational resources. Feel free to\n",
    "try more interations on your own.\n",
    "\n",
    "Once the `RandomizedSearchCV` has found the best set of hyperparameters, it\n",
    "uses them to refit the model using the full training set. To estimate the\n",
    "generalization performance of the best model it suffices to call `.score` on\n",
    "the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, our random forest regressor makes an error of 34.00 k$\n"
     ]
    }
   ],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(\n",
    "    f\"On average, our random forest regressor makes an error of {error:.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a114c7b",
   "metadata": {},
   "source": [
    "## Histogram gradient-boosting decision trees\n",
    "\n",
    "For gradient-boosting, hyperparameters are coupled, so we cannot set them\n",
    "one after the other anymore. The important hyperparameters are `max_iter`,\n",
    "`learning_rate`, and `max_depth` or `max_leaf_nodes` (as previously discussed\n",
    "random forest).\n",
    "\n",
    "Let's first discuss `max_iter` which, similarly to the `n_estimators`\n",
    "hyperparameter in random forests, controls the number of trees in the\n",
    "estimator. The difference is that the actual number of trees trained by the\n",
    "model is not entirely set by the user, but depends also on the stopping\n",
    "criteria: the number of trees can be lower than `max_iter` if adding a new\n",
    "tree does not improve the model enough. We will give more details on this in\n",
    "the next exercise.\n",
    "\n",
    "The depth of the trees is controlled by `max_depth` (or `max_leaf_nodes`). We\n",
    "saw in the section on gradient-boosting that boosting algorithms fit the error\n",
    "of the previous tree in the ensemble. Thus, fitting fully grown trees would be\n",
    "detrimental. Indeed, the first tree of the ensemble would perfectly fit\n",
    "(overfit) the data and thus no subsequent tree would be required, since there\n",
    "would be no residuals. Therefore, the tree used in gradient-boosting should\n",
    "have a low depth, typically between 3 to 8 levels, or few leaves ($2^3=8$ to\n",
    "$2^8=256$). Having very weak learners at each step helps reducing overfitting.\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the\n",
    "residuals are corrected and then less learners are required. Therefore,\n",
    "it can be beneficial to increase `max_iter` if `max_depth` is low.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter until\n",
    "now. When fitting the residuals, we would like the tree to try to correct all\n",
    "possible errors or only a fraction of them. The learning-rate allows you to\n",
    "control this behaviour. A small learning-rate value would only correct the\n",
    "residuals of very few samples. If a large learning-rate is set (e.g., 1), we\n",
    "would fit the residuals of all samples. So, with a very low learning-rate, we\n",
    "would need more estimators to correct the overall error. However, a too large\n",
    "learning-rate tends to obtain an overfitted ensemble, similar to having very\n",
    "deep trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>30.872935</td>\n",
       "      <td>0.292334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>20</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>31.750762</td>\n",
       "      <td>0.336768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>0.176656</td>\n",
       "      <td>32.407865</td>\n",
       "      <td>0.369394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>0.297739</td>\n",
       "      <td>32.883823</td>\n",
       "      <td>0.562385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.083745</td>\n",
       "      <td>33.139713</td>\n",
       "      <td>0.268990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.215543</td>\n",
       "      <td>33.334051</td>\n",
       "      <td>0.157767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>33.688241</td>\n",
       "      <td>0.447378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05929</td>\n",
       "      <td>35.830092</td>\n",
       "      <td>0.480495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.160519</td>\n",
       "      <td>36.314363</td>\n",
       "      <td>0.482098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125207</td>\n",
       "      <td>40.750639</td>\n",
       "      <td>0.653421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.054511</td>\n",
       "      <td>42.160628</td>\n",
       "      <td>0.667869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.248463</td>\n",
       "      <td>50.133962</td>\n",
       "      <td>0.888701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>50.135783</td>\n",
       "      <td>1.136707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>61.590128</td>\n",
       "      <td>0.767126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079415</td>\n",
       "      <td>81.467519</td>\n",
       "      <td>0.906560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>82.513416</td>\n",
       "      <td>0.946309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>87.702993</td>\n",
       "      <td>1.055620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>87.864160</td>\n",
       "      <td>1.133426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>88.364966</td>\n",
       "      <td>1.012884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01724</td>\n",
       "      <td>88.861417</td>\n",
       "      <td>1.117387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_max_iter param_max_leaf_nodes param_learning_rate  mean_test_error  \\\n",
       "14            300                  100             0.01864        30.872935   \n",
       "6             300                   20            0.047293        31.750762   \n",
       "2              30                   50            0.176656        32.407865   \n",
       "13            300                   10            0.297739        32.883823   \n",
       "9             100                   20            0.083745        33.139713   \n",
       "19            100                   10            0.215543        33.334051   \n",
       "12            100                   20            0.067503        33.688241   \n",
       "16            300                    5             0.05929        35.830092   \n",
       "1             100                    5            0.160519        36.314363   \n",
       "0            1000                    2            0.125207        40.750639   \n",
       "7            1000                    2            0.054511        42.160628   \n",
       "18             10                    5            0.248463        50.133962   \n",
       "8               3                    5            0.906226        50.135783   \n",
       "5              10                  100            0.061034        61.590128   \n",
       "17              3                    5            0.079415        81.467519   \n",
       "4              10                    2              0.0351        82.513416   \n",
       "15              3                   50            0.019923        87.702993   \n",
       "3               3                    2            0.039361        87.864160   \n",
       "11              3                   10            0.019351        88.364966   \n",
       "10              3                    5             0.01724        88.861417   \n",
       "\n",
       "    std_test_error  \n",
       "14        0.292334  \n",
       "6         0.336768  \n",
       "2         0.369394  \n",
       "13        0.562385  \n",
       "9         0.268990  \n",
       "19        0.157767  \n",
       "12        0.447378  \n",
       "16        0.480495  \n",
       "1         0.482098  \n",
       "0         0.653421  \n",
       "7         0.667869  \n",
       "18        0.888701  \n",
       "8         1.136707  \n",
       "5         0.767126  \n",
       "17        0.906560  \n",
       "4         0.946309  \n",
       "15        1.055620  \n",
       "3         1.133426  \n",
       "11        1.012884  \n",
       "10        1.117387  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"max_iter\": [3, 10, 30, 100, 300, 1000],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": loguniform(0.01, 1),\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    HistGradientBoostingRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_iter=20,\n",
    "    random_state=0,\n",
    "    n_jobs=2,\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1298f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Here, we tune <tt class=\"docutils literal\">max_iter</tt> but be aware that it is better to set <tt class=\"docutils literal\">max_iter</tt> to a\n",
    "fixed, large enough value and use parameters linked to <tt class=\"docutils literal\">early_stopping</tt> as we\n",
    "will do in Exercise M6.04.</p>\n",
    "</div>\n",
    "\n",
    "In this search, we observe that for the best ranked models, having a\n",
    "smaller `learning_rate`, requires more trees or a larger number of leaves\n",
    "for each tree. However, it is particularly difficult to draw more detailed\n",
    "conclusions since the best value of each hyperparameter depends on the other\n",
    "hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c6ac8",
   "metadata": {},
   "source": [
    "We can now estimate the generalization performance of the best model using the\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad0e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, our HGBT regressor makes an error of 30.48 k$\n"
     ]
    }
   ],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"On average, our HGBT regressor makes an error of {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d9c6d",
   "metadata": {},
   "source": [
    "The mean test score in the held-out test set is slightly better than the score\n",
    "of the best model. The reason is that the final model is refitted on the whole\n",
    "training set and therefore, on more data than the cross-validated models of\n",
    "the grid search procedure.\n",
    "\n",
    "We summarize these details in the following table:\n",
    "\n",
    "| **Bagging & Random Forests**                     | **Boosting**                                        |\n",
    "|--------------------------------------------------|-----------------------------------------------------|\n",
    "| fit trees **independently**                      | fit trees **sequentially**                          |\n",
    "| each **deep tree overfits**                      | each **shallow tree underfits**                     |\n",
    "| averaging the tree predictions **reduces overfitting** | sequentially adding trees **reduces underfitting** |\n",
    "| generalization improves with the number of trees | too many trees may cause overfitting                |\n",
    "| does not have a `learning_rate` parameter        | fitting the residuals is controlled by the `learning_rate` |"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/ensemble_hyperparameters.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
