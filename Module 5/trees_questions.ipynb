{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ames_housing = pd.read_csv(\n",
    "    \"../datasets/ames_housing_no_missing.csv\",\n",
    "    na_filter=False,  # required for pandas>2.0\n",
    ")\n",
    "target_name = \"SalePrice\"\n",
    "data = ames_housing.drop(columns=target_name)\n",
    "target = ames_housing[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "    \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n",
    "    \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
    "]\n",
    "\n",
    "data_numerical = data[numerical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the linear model is better in 9 situations\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "linear_model = make_pipeline(\n",
    "    StandardScaler(), LinearRegression()\n",
    ")\n",
    "\n",
    "cv_results_linear = cross_validate(\n",
    "    linear_model,\n",
    "    data_numerical,\n",
    "    target,\n",
    "    cv=10,\n",
    "    # scoring=\"neg_mean_squared_error\",\n",
    "    # return_estimator=True,\n",
    ")\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "cv_results_tree = cross_validate(\n",
    "    tree,\n",
    "    data_numerical,\n",
    "    target,\n",
    "    cv=10,\n",
    "    # scoring=\"neg_mean_squared_error\",\n",
    "    # return_estimator=True,\n",
    ")\n",
    "\n",
    "count = 0\n",
    "for linear_score, tree_score in zip(cv_results_linear[\"test_score\"], cv_results_tree[\"test_score\"]):\n",
    "    if (linear_score > tree_score):\n",
    "        count += 1\n",
    "        \n",
    "print(f'the linear model is better in {count} situations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for fold #1:\n",
      "{'max_depth': 5}\n",
      "Best hyperparameters for fold #2:\n",
      "{'max_depth': 7}\n",
      "Best hyperparameters for fold #3:\n",
      "{'max_depth': 6}\n",
      "Best hyperparameters for fold #4:\n",
      "{'max_depth': 6}\n",
      "Best hyperparameters for fold #5:\n",
      "{'max_depth': 8}\n",
      "Best hyperparameters for fold #6:\n",
      "{'max_depth': 6}\n",
      "Best hyperparameters for fold #7:\n",
      "{'max_depth': 7}\n",
      "Best hyperparameters for fold #8:\n",
      "{'max_depth': 8}\n",
      "Best hyperparameters for fold #9:\n",
      "{'max_depth': 7}\n",
      "Best hyperparameters for fold #10:\n",
      "{'max_depth': 6}\n",
      "the linear model is better in 8 situations\n",
      "A tree with an optimized depth is better than linear regression for 2 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(1, 16)}\n",
    "tree_reg = GridSearchCV(tree, param_grid=param_grid, cv=10)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    tree_reg, data_numerical, target, cv=10, n_jobs=2, return_estimator=True\n",
    ")\n",
    "\n",
    "for cv_fold, estimator_in_fold in enumerate(cv_results[\"estimator\"]):\n",
    "    print(\n",
    "        f\"Best hyperparameters for fold #{cv_fold + 1}:\\n\"\n",
    "        f\"{estimator_in_fold.best_params_}\"\n",
    "    )\n",
    "\n",
    "\n",
    "count = 0\n",
    "for linear_score, tree_score in zip(cv_results_linear[\"test_score\"], cv_results[\"test_score\"]):\n",
    "    if (linear_score > tree_score):\n",
    "        count += 1\n",
    "        \n",
    "print(f'the linear model is better in {count} situations')\n",
    "\n",
    "print(\n",
    "    \"A tree with an optimized depth is better than linear regression for \"\n",
    "    f'{sum(cv_results[\"test_score\"] > cv_results_linear[\"test_score\"])} CV '\n",
    "    \"iterations out of 10 folds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree with numerical and categorical data is better than linear regression for 6 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "# new_data = data.drop(columns=\"GarageArea\")\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"categorical\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_columns),\n",
    "    ('numerical', \"passthrough\", numerical_features)], #no need to scale numerical data for decision tree,\n",
    ") #by default it drops the other columns \n",
    "\n",
    "\n",
    "new_tree = make_pipeline(\n",
    "    preprocessor,\n",
    "    DecisionTreeRegressor(max_depth=7),\n",
    ")\n",
    "\n",
    "cv_results_new = cross_validate(\n",
    "    new_tree, data, target, cv=10, n_jobs=2, return_estimator=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    \"A tree with numerical and categorical data is better than linear regression for \"\n",
    "    f'{sum(cv_results_new[\"test_score\"] > cv_results[\"test_score\"])} CV '\n",
    "    \"iterations out of 10 folds.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
